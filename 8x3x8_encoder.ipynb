{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fucking_Neural_network(object):\n",
    "    ''' Defining the neural network class'''\n",
    "    def __init__(self):\n",
    "        # Parameters that do not change (hyperparameters)\n",
    "        self.inputLayerSize = 8\n",
    "        self.outputLayerSize = 8\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        # Initializing weights\n",
    "        self.w1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.w2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Forward propagation\n",
    "        self.z2 = np.dot(x, self.w1) # inputs x synapse weights 1\n",
    "        self.a2 = fucking_sigmoid(np.array(self.z2)) # apply activation function\n",
    "\n",
    "        self.z3 = np.dot(self.a2, self.w2) # output of layer x synapse weights 2\n",
    "        y_hat = fucking_sigmoid(self.z3) # apply activation function again\n",
    "        return y_hat\n",
    "\n",
    "NN = fucking_Neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fucking_sigmoid(z):\n",
    "    # Applying a sigmoidal activation function\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoidPrime(z):\n",
    "    # Derivative of above sigmoid function\n",
    "    return np.exp(-z)/((1 + np.exp(-z))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costFunction(X,y):\n",
    "    # Using the sum of the squared error as cost with an added regularization factor \n",
    "    NN.y_hat = NN.forward(X)\n",
    "    cost = 0.5*sum((y-NN.y_hat)**2)/X.shape[0] + (Lambda/2)*(np.sum(NN.w1**2) + sum(NN.w2**2)) \n",
    "    return cost\n",
    "\n",
    "def costFunctionPrime(X, y):\n",
    "    # Derivative with respect to w1 and w2\n",
    "    NN.y_hat = NN.forward(X)\n",
    "    \n",
    "    delta3 = np.multiply(-(y - NN.y_hat), sigmoidPrime(NN.z3))\n",
    "    print(delta3.shape, y.shape, sigmoidPrime(NN.z3).shape)\n",
    "    dJdw2 = np.dot(NN.a2.T, delta3) + Lambda*NN.w2\n",
    "    delta2 = np.multiply(delta3, NN.w2.T) * (sigmoidPrime(NN.z2))\n",
    "    dJdw1 = np.dot(X.T, delta2) + Lambda*NN.w1\n",
    "    \n",
    "    return dJdw1, dJdw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParams():\n",
    "    # Get w1 and w2 as vectors\n",
    "    params = np.concatenate((NN.w1.ravel(), NN.w2.ravel()))\n",
    "    return params\n",
    "\n",
    "def setParams(params):\n",
    "    # Take w as vectors and set new w1 and w2\n",
    "    w1_start = 0\n",
    "    w1_end = NN.hiddenLayerSize * NN.inputLayerSize\n",
    "    NN.w1 = np.reshape(params[w1_start:w1_end], (NN.inputLayerSize, NN.hiddenLayerSize))\n",
    "    w2_end = w1_end + NN.hiddenLayerSize*NN.outputLayerSize\n",
    "    NN.w2 = np.reshape(params[w1_end:w2_end], (NN.hiddenLayerSize, NN.outputLayerSize))\n",
    "\n",
    "def computeGradient(X,y):\n",
    "    # Computes a gradient (duh)\n",
    "    dJdw1, dJdw2 = costFunctionPrime(X,y)\n",
    "    return np.concatenate((dJdw1.ravel(), dJdw2.ravel()))\n",
    "    \n",
    "def newGradient(X, y, learningRate):  \n",
    "    theta = np.zeros((8,8))\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "\n",
    "    parameters = int(NN.inputLayerSize*NN.hiddenLayerSize + NN.hiddenLayerSize*NN.outputLayerSize )\n",
    "    grad = np.zeros(parameters)\n",
    "    error = fucking_sigmoid(X * theta.T) - y\n",
    "\n",
    "    grad = ((X.T * error) / len(X)).T + ((learningRate / len(X)) * theta)\n",
    "    # intercept gradient is not regularized\n",
    "    grad[0, 0] = np.sum(np.multiply(error, X[:,0])) / len(X)\n",
    "\n",
    "    return np.array(grad).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNumericalGradient(N,X,y):\n",
    "    ''' Estimating the gradient numerically to compare values. '''\n",
    "    paramsInitial = getParams()\n",
    "    numgrad = np.zeros(paramsInitial.shape)\n",
    "    perturb = np.zeros(paramsInitial.shape)\n",
    "    e = 1e-4\n",
    "    \n",
    "    for p in range(len(paramsInitial)):\n",
    "        # Set perturbation vector\n",
    "        perturb[p] = e\n",
    "        setParams(paramsInitial + perturb)\n",
    "        loss2 = costFunction(X,y)\n",
    "        \n",
    "        setParams(paramsInitial - perturb)\n",
    "        loss1 = costFunction(X,y)\n",
    "        \n",
    "        # Compute num grad\n",
    "        print ((loss2 - loss1)/(2*e))\n",
    "        numgrad[p] = (loss2 - loss1)/(2*e)\n",
    "        \n",
    "        #Return changed value to zero\n",
    "        perturb[p] = 0\n",
    "    \n",
    "    # Return params to original values\n",
    "    setParams(paramsInitial)\n",
    "\n",
    "    return numgrad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Sample execution with test values '''\n",
    "X = np.identity(8)\n",
    "y = np.identity(8)\n",
    "\n",
    "# Regularization parameter\n",
    "Lambda = 0.0003\n",
    "\n",
    "learningRate = 0.5\n",
    "\n",
    "testRun = NN.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRunClearer = np.round(testRun, decimals=0) # Visualize how close am I to the correct output\n",
    "print(testRunClearer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,) (8,)\n"
     ]
    }
   ],
   "source": [
    "cost1 = costFunction(X,y)\n",
    "#dJdw1, dJdw2 = costFunctionPrime(X,y)\n",
    "\n",
    "# When we account for derivatives (by subtracting them) the cost should go down\n",
    "\n",
    "#NN.w1 = NN.w1 - 3*dJdw1\n",
    "#NN.w2 = NN.w2 - 3*dJdw2\n",
    "cost3 = costFunction(X,y)\n",
    "\n",
    "print (cost1.shape, cost3.shape)\n",
    "# The problem is I have 8 costs and the reason is that my initial activation function calculation\n",
    "# is multiplying an (8,8) by an (8,3) giving me 8 scalar costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.375   0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625\n",
      " -0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625\n",
      " -0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625\n",
      " -0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625\n",
      " -0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625\n",
      " -0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625\n",
      " -0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625  0.0625\n",
      " -0.0625]\n"
     ]
    }
   ],
   "source": [
    "newgrad = newGradient(X, y, learningRate)\n",
    "print(newgrad)\n",
    "\n",
    "#numgrad = computeNumericalGradient(NN, X, y)\n",
    "#grad = computeGradient(X,y)\n",
    "\n",
    "# This measures how similar they are (should be < 10^8)\n",
    "#np.linalg.norm(grad-newgrad) / np.linalg.norm(grad+newgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "    ''' Defining a training class. Modified to check testing error during training. '''\n",
    "    def __init__(self, N):\n",
    "        # Make a local reference to the neural network\n",
    "        self.N = N    \n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        setParams(params)\n",
    "        self.J.append(costFunction(self.X, self.y))\n",
    "        self.testJ.append(costFunction(self.testX, self.testy))\n",
    "    \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        # To track the cost function over training time\n",
    "        setParams(params)\n",
    "        cost = costFunction(X, y)\n",
    "#        grad = computeGradient(X, y)\n",
    "        grad = newGradient(X, y, learningRate)\n",
    "        print(\"Grad:\", grad.shape)\n",
    "        return cost, grad\n",
    "    \n",
    "    # This is the actual training function\n",
    "    def train(self, trainX, trainy, testX, testy):\n",
    "        # Make internal variable for callback\n",
    "        self.X = trainX\n",
    "        self.y = trainy\n",
    "        \n",
    "        self.testX = testX\n",
    "        self.testy = testy\n",
    "        \n",
    "        # Make empty list to store costs\n",
    "        self.J = []\n",
    "        self.testJ = []\n",
    "        params0 = getParams()\n",
    "        print(\"Params:\", params0.shape)\n",
    "        \n",
    "        options = {'maxiter': 200, 'disp': True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac = True, method = 'BFGS', args = (X,y), options = options, callback = self.callbackF)\n",
    "        \n",
    "        # Update params with new values from last iteration\n",
    "        setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Execution with Training and Testing values. \\\n",
    "    Using the first half of the 8x8 identity matrix as a training set \\\n",
    "    and the second half as a testing set. '''\n",
    "\n",
    "trainX = np.identity(8)[0:4]\n",
    "trainy = np.identity(8)[0:4]\n",
    "testX = np.identity(8)[4:8]\n",
    "testy = np.identity(8)[4:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: (48,)\n",
      "Grad: (64,)\n",
      "Grad: (64,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (48,48) and (64,) not aligned: 48 (dim 1) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-d06534197954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-119-b6ffbbe0f459>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainX, trainy, testX, testy)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'disp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0m_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcostFunctionWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'BFGS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbackF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Update params with new values from last iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0mgnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvecnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgfk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgnorm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mgtol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         \u001b[0mpk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (48,48) and (64,) not aligned: 48 (dim 1) != 64 (dim 0)"
     ]
    }
   ],
   "source": [
    "T = trainer(NN)\n",
    "T.train(trainX, trainy, testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T.J)\n",
    "plt.plot(T.testJ)\n",
    "plt.grid(1)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend(['Training', 'Testing'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Final test using random data'''\n",
    "\n",
    "hoursSleep = np.linspace(0,10,100)\n",
    "hoursStudy = np.linspace(0,5,100)\n",
    "\n",
    "# Normalize\n",
    "hoursSleepNorm = hoursSleep/10.\n",
    "hoursStudyNorm = hoursStudy/5.\n",
    "\n",
    "# Make into 2D arrays\n",
    "a, b = np.meshgrid(hoursSleepNorm, hoursStudyNorm)\n",
    "\n",
    "# Merge into single matrix\n",
    "allInputs = np.zeros((a.size,2))\n",
    "allInputs[:,0]: a.ravel()\n",
    "allInputs[:,1]: b.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allOutputs = NN.forward(allInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
